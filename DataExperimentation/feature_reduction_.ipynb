{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1b8ef928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "224f046e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Planetoid(root = 'data', name = 'cora')\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "642c9a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47359262",
   "metadata": {},
   "source": [
    "**Feature Manipulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b9c99c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1424"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.x) # 2708 -> rows or nodes\n",
    "len(data.x[0])  # 1433 -> bag of word encoding dim or feture dim\n",
    "len(data.x[0][data.x[0] == 1]) # 9 -> only 9 words of 1433 are included\n",
    "len(data.x[0][data.x[0] == 0]) # 1424"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "25876813",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_high = []\n",
    "lst_low = []\n",
    "for i in range(len(data.x)):\n",
    "  n = len(data.x[i][data.x[i] > 0])\n",
    "  lst_high.append(n)\n",
    "  lst_low.append(1433 - n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f92f8173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9, 23, 19, 21, 18, 13, 18, 14, 20,  3])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "highs = np.array(lst_high)\n",
    "np.max(highs)\n",
    "highs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1a8097a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.x\n",
    "num_features = X.size(1) # embedding dim -> 1433\n",
    "word_counts = torch.sum(X, dim = 0)\n",
    "counts = [(i, word_counts[i].item()) for i in range(num_features)]\n",
    "counts_sorted = sorted(counts, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "def normalize_features(X):\n",
    "  row_sum = X.sum(dim = 1, keepdim = True)\n",
    "  X = X/row_sum.clamp(min = 1)\n",
    "  return X\n",
    "\n",
    "# choosing top k\n",
    "def choose_top_k(k = 600, data = data, X = X):\n",
    "  top_k_indices = [idx for idx, freq in counts_sorted[:k]]\n",
    "\n",
    "  # filter feature matrix\n",
    "  x_top_k = X[:, top_k_indices] # new embeddings -> 2708 x k\n",
    "\n",
    "  lst_high_top = []\n",
    "  lst_low_top = []\n",
    "  for i in range(len(data.x)):\n",
    "    n = len(x_top_k[i][x_top_k[i] == 1])\n",
    "    lst_high_top.append(n)\n",
    "    lst_low_top.append(1433 - n)\n",
    "  x_top_k = normalize_features(x_top_k)\n",
    "\n",
    "  return x_top_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50012139",
   "metadata": {},
   "source": [
    "**Building Adjacency Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c65e0de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 2708])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_adjacency(data):\n",
    "  N = data.num_nodes \n",
    "  edge_index =  data.edge_index\n",
    "\n",
    "  A = torch.zeros((N,N))\n",
    "  A[edge_index[0], edge_index[1]] = 1\n",
    "  A = A + A.T\n",
    "  A[A > 1] = 1\n",
    "  return A\n",
    "\n",
    "def normalise_adj(A):\n",
    "  N = A.size(0)\n",
    "  I = torch.eye(N)\n",
    "  A_hat = A + I # adding self loops\n",
    "\n",
    "  D_hat = torch.diag(A_hat.sum(dim = 1))\n",
    "  D_hat_inv_sqrt = torch.linalg.inv(torch.sqrt(D_hat))\n",
    "\n",
    "  A_norm = D_hat_inv_sqrt @ A_hat @ D_hat_inv_sqrt\n",
    "  return A_norm\n",
    "\n",
    "A = build_adjacency(data)\n",
    "A_norm = normalise_adj(A)\n",
    "A_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4251e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "  def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "    super().__init__()\n",
    "    self.W1 = torch.nn.Parameter(torch.randn(in_dim, hidden_dim))\n",
    "    self.W2 = torch.nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
    "    self.W3 = torch.nn.Parameter(torch.randn(hidden_dim, out_dim))\n",
    "\n",
    "  def forward(self, A_norm , X):\n",
    "    H = torch.relu(A_norm @ X @ self.W1) # 1st Aggreation and projection\n",
    "    H = torch.relu(A_norm @ H @ self.W2) # 2nd Aggregation & projection\n",
    "    H = A_norm @ H @ self.W3 # outputs logits for num_classes\n",
    "    return H # logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2e4da1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n",
      "tensor(6)\n",
      "torch.int64\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "labels_cpu = data.y.cpu()\n",
    "\n",
    "print(labels_cpu.min())\n",
    "print(labels_cpu.max())\n",
    "print(labels_cpu.dtype)\n",
    "print(dataset.num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6794a21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "TRAIN_ACC = []\n",
    "TEST_ACC = []\n",
    "k_val = [200,300,400,500,600,700,800,1000]\n",
    "\n",
    "A_norm = A_norm.to(device)\n",
    "labels = data.y.to(device)\n",
    "labels = data.y.long().to(device)\n",
    "\n",
    "train_mask = data.train_mask.to(device)\n",
    "test_mask = data.test_mask.to(device)\n",
    "\n",
    "for k_ in k_val: \n",
    "  x_top_k = choose_top_k(k= k_)\n",
    "  model = GCN(in_dim = x_top_k.size(1),hidden_dim = 16,out_dim = dataset.num_classes).to(device)\n",
    "\n",
    "  X = x_top_k.to(device)\n",
    "\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3, weight_decay = 1e-2)\n",
    "  loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "  train_acc = 0\n",
    "  for epoch in range(5000):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out = model(A_norm, X)\n",
    "    loss = loss_fn(out[train_mask], labels[train_mask])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # if epoch % 20 == 0:\n",
    "    with torch.no_grad():\n",
    "      pred = out.argmax(dim=1)\n",
    "      train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
    "    # print(f\"Epoch {epoch} | Loss {loss:.4f} | Train Acc {acc:.3f}\")\n",
    "  TRAIN_ACC.append(train_acc.item())\n",
    "\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "      out = model(A_norm, X)\n",
    "      pred = out.argmax(dim=1)\n",
    "      test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
    "      # print(\"Test Accuracy:\", test_acc.item())\n",
    "  TEST_ACC.append(test_acc.item())\n",
    "  del X, x_top_k, out, loss, pred, model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "869fa3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200, 300, 400, 500, 600, 700, 800, 1000]\n",
      "[0.9428571462631226, 0.9285714626312256, 0.9142857193946838, 0.9357143044471741, 0.9357143044471741, 0.9142857193946838, 0.9428571462631226, 0.9214285612106323]\n",
      "['0.76', '0.77', '0.77', '0.77', '0.78', '0.74', '0.77', '0.77']\n"
     ]
    }
   ],
   "source": [
    "print(k_val)\n",
    "print(TRAIN_ACC)\n",
    "print([f\"{acc:.2f}\" for acc in TEST_ACC])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85802852",
   "metadata": {},
   "source": [
    "### GCN using Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56318a8e",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c70027e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44645160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN_lib(torch.nn.Module):\n",
    "  def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "    super().__init__()\n",
    "    self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "    self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "    self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "  def forward(self, x, edge_index):\n",
    "    x = self.conv1(x, edge_index)\n",
    "    x = F.relu(x)\n",
    "    x = self.conv2(x, edge_index)\n",
    "    x = F.relu(x)\n",
    "    x = self.conv3(x, edge_index)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77543070",
   "metadata": {},
   "source": [
    "**Initializing Model & Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "67dcbc3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.to(device)\n",
    "def train(model, X, data, optimizer, labels, train_mask):\n",
    "  model.train()\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  out = model(X, data.edge_index)\n",
    "  loss = F.cross_entropy(out[train_mask], labels[train_mask])\n",
    "\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  return loss.item()\n",
    "\n",
    "def test(model, X, data, labels, test_mask):\n",
    "  model.eval()\n",
    "  out = model(X, data.edge_index)\n",
    "  pred = out.argmax(dim=1)\n",
    "\n",
    "  correct = (pred[test_mask] == labels[data.test_mask]).sum()\n",
    "  acc = int(correct) / int(data.test_mask.sum())\n",
    "  return acc\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5d3a961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TRAIN_ACC = []\n",
    "TEST_ACC = []\n",
    "\n",
    "labels = data.y.to(device)\n",
    "labels = data.y.long().to(device)\n",
    "\n",
    "train_mask = data.train_mask.to(device)\n",
    "test_mask = data.test_mask.to(device)\n",
    "\n",
    "k_val = [200,300,400,500,600,700,800,1000, 1400]\n",
    "\n",
    "for k_ in k_val: \n",
    "  x_top_k = choose_top_k(k= k_).to(device)\n",
    "  model = GCN_lib(in_channels = x_top_k.size(1),hidden_channels = 16, out_channels = dataset.num_classes).to(device)\n",
    "\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2, weight_decay = 5e-4)\n",
    "\n",
    "  for epochs in range(5000):\n",
    "    loss = train(model, x_top_k, data, optimizer, labels, train_mask)\n",
    "  acc = test(model, x_top_k, data, labels, test_mask)\n",
    "  TEST_ACC.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f36d88f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200, 300, 400, 500, 600, 700, 800, 1000, 1400]\n",
      "[]\n",
      "['0.74', '0.75', '0.76', '0.79', '0.76', '0.79', '0.80', '0.76', '0.77']\n"
     ]
    }
   ],
   "source": [
    "print(k_val)\n",
    "print(TRAIN_ACC)\n",
    "print([f\"{acc:.2f}\" for acc in TEST_ACC])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
